# pull image by default
CITOOL_EXTRA_PODMAN_ARGS ?= --pull newer

# default worker image
WORKER_IMAGE ?= quay.io/testing-farm/worker:latest

# default development cluster name
DEV_CLUSTER_NAME ?= $(or $(TF_VAR_cluster_name),testing-farm-dev-$$USER)

# run pytest tests in parallel
PYTEST_PARALLEL_RUNS ?= 16
PYTEST_PARALLEL_OPTIONS ?= -d --tx $(PYTEST_PARALLEL_RUNS)*popen//python=python3.9

# default Terraform lock timeout
DEFAULT_LOCK_TIMEOUT ?= 10m

# Testing Farm production API
TESTING_FARM_PRODUCTION_API_URL ?= $(TESTING_FARM_API_URL)
TESTING_FARM_PRODUCTION_API_TOKEN ?= $(TESTING_FARM_API_TOKEN_PUBLIC)

# For scale-redhat-worker target, make all additional targets parameters
# https://stackoverflow.com/questions/2214575/passing-arguments-to-make-run
ifneq (,$(filter $(firstword $(MAKECMDGOALS)),create/public/worker create/redhat/worker))
  # use the rest as arguments for "SCALE_TARGET"
  SCALE_TARGET := $(wordlist 2,$(words $(MAKECMDGOALS)),$(MAKECMDGOALS))
  SCALE_TARGET := $(if $(SCALE_TARGET),$(SCALE_TARGET),1)
  # ...and turn them into do-nothing targets
  $(eval $(SCALE_TARGET):;@:)
endif

# Help prelude
define PRELUDE

Usage:
  make [target]

Variables defaults:

  WORKER_IMAGE = $(WORKER_IMAGE)     âš™ï¸  test/* targets

endef


##@ Infrastructure | Infra (Gitlab, etc.)

define run_terragrunt
	TERRAGRUNT_WORKING_DIR=terragrunt/environments/$1 terragrunt run-all $2 --terragrunt-non-interactive -lock-timeout $(DEFAULT_LOCK_TIMEOUT)
endef

infra/init: .FORCE  ## Initialize | infra
	$(call run_terragrunt,infra,init)

infra/plan: .FORCE  ## Plan deployment | infra
	$(call run_terragrunt,infra,plan)

infra/apply: .FORCE  ## Deploy | infra
	$(call run_terragrunt,infra,apply)

infra/destroy: .FORCE  ## Destroy | infra
	$(call run_terragrunt,infra,destroy)


##@ Infrastructure | Dev

dev/init: .FORCE  ## Initialize | dev | all
	$(call run_terragrunt,dev,init)

dev/plan: .FORCE  ## Plan deployment | dev | all
	$(call run_terragrunt,dev,plan)

dev/plan/eks: .FORCE  ## Plan deployment | dev | eks
	$(call run_terragrunt,dev/eks,plan)

dev/plan/artemis: .FORCE  ## Plan deployment | dev | artemis
	$(call run_terragrunt,dev/artemis,plan)

dev/plan/server: .FORCE  ## Plan deployment | dev | server
	$(call run_terragrunt,dev/server,plan)

dev/apply: .FORCE  ## Deploy | dev | all
	# workaround for terragrunt bug having issues with dependencies
	$(call run_terragrunt,dev/server/security-group,apply)
	$(call run_terragrunt,dev,apply)

dev/apply/eks: .FORCE  ## Deploy | dev | eks
	$(call run_terragrunt,dev/eks,apply)

dev/apply/artemis: .FORCE  ## Deploy | dev | artemis
	$(call run_terragrunt,dev/artemis,apply)

dev/apply/server: .FORCE  ## Deploy | dev | server
	$(call run_terragrunt,dev/server,apply)

dev/destroy: .FORCE terminate/artemis/guests/dev  ## Destroy | dev | all
	$(call run_terragrunt,dev,destroy)

dev/kubeconfig: .FORCE  ## Update kubeconfig for development environemnt
	aws eks --profile fedora_us_east_2 update-kubeconfig --name $(DEV_CLUSTER_NAME)


##@ Infrastructure | Staging

staging/init: .FORCE  ## Initialize | staging | all
	$(call run_terragrunt,staging,init)

staging/init/artemis/ci: .FORCE  ## Initialize | staging | artemis | CI
	$(call run_terragrunt,staging/artemis-ci,init)

staging/plan: .FORCE  ## Plan deployment | staging
	$(call run_terragrunt,staging,plan)

staging/plan/eks: .FORCE  ## Plan deployment | staging | eks
	$(call run_terragrunt,staging/eks,plan)

staging/plan/artemis: .FORCE  ## Plan deployment | staging | artemis
	$(call run_terragrunt,staging/artemis,plan)

staging/plan/artemis/ci: .FORCE  ## Plan deployment | staging | artemis | CI
	$(call run_terragrunt,staging/artemis-ci,plan)

staging/apply: .FORCE  ## Deploy | staging | all
	$(call run_terragrunt,staging,apply)

staging/apply/eks: .FORCE  ## Deploy | staging | eks
	$(call run_terragrunt,staging/eks,apply)

staging/apply/artemis: .FORCE  ## Deploy | staging | artemis
	$(call run_terragrunt,staging/artemis,apply)

staging/apply/artemis/ci: .FORCE  ## Deploy | staging | artemis | CI
	$(call run_terragrunt,staging/artemis-ci,apply)

staging/destroy: .FORCE terminate/artemis/guests/staging  ## Destroy | staging
	$(call run_terragrunt,staging,destroy)

staging/destroy/artemis/ci: .FORCE terminate/artemis/guests/staging/ci  ## Destroy | staging | artemis | CI
	$(call run_terragrunt,staging/artemis-ci,destroy)

staging/kubeconfig: .FORCE  ## Update kubeconfig for staging environment
	aws eks --profile fedora_us_east_1 update-kubeconfig --name testing-farm-staging


##@ Infrastructure | Production

production/init: .FORCE  ## Initialize | production | all
	$(call run_terragrunt,production,init)

production/plan: .FORCE  ## Plan deployment | production
	$(call run_terragrunt,production,plan)

production/plan/eks: .FORCE  ## Plan deployment | production | eks
	$(call run_terragrunt,production/eks,plan)

production/plan/artemis: .FORCE  ## Plan deployment | production | artemis
	$(call run_terragrunt,production/artemis,plan)

production/apply: .FORCE  ## Deploy | production | all
	$(call run_terragrunt,production,apply)

production/apply/eks: .FORCE  ## Deploy | production | eks
	$(call run_terragrunt,production/eks,apply)

production/apply/artemis: .FORCE  ## Deploy | production | artemis
	$(call run_terragrunt,production/artemis,apply)

production/destroy: .FORCE terminate/artemis/guests/production  ## Destroy | production
	$(call run_terragrunt,production,destroy)

production/kubeconfig: .FORCE  ## Update kubeconfig for production environment
	aws eks --profile fedora_us_east_1 update-kubeconfig --name testing-farm-production

.create/redhat/worker%:
	ansible-playbook -v ansible/playbooks/create-redhat-worker.yaml

create/redhat/worker: .FORCE  ## Create a Red Hat ranch worker, use `make create/redhat/worker N` to create N workers
	@echo -e "ðŸš€ \033[0;32mcreating $(SCALE_TARGET) redhat worker(s)\033[0m"
	@$(MAKE) -j $(addprefix .create/redhat/worker, $(shell seq 1 $(SCALE_TARGET)))

.create/public/worker%:
	ansible-playbook -v ansible/playbooks/create-public-worker.yaml

create/public/worker: .FORCE  ## Create a Public ranch worker, use `make create/public/worker N` to create N workers
	@echo -e "ðŸš€ \033[0;32mcreating $(SCALE_TARGET) public worker(s)\033[0m"
	@$(MAKE) -j $(addprefix .create/public/worker, $(shell seq 1 $(SCALE_TARGET)))

define run_pipeline_update
	@echo -e "$(1) \033[0;32m$(2)\033[0m"
	ansible-playbook -v -l $(3) -t $(4) ansible/playbooks/testing-farm.yaml
endef

# To pass a comma in the call command arguments
# https://www.gnu.org/software/make/manual/html_node/Syntax-of-Functions.html
comma := ,

update/pipeline/image: .FORCE  ## Update pipeline container image on all ranches
	$(call run_pipeline_update,ðŸ’¿ï¸,updating pipeline image on all ranches,testing_farm_public_workers$(comma)testing_farm_redhat_workers,update_image)

update/public/pipeline/image: .FORCE  ## Update pipeline container image on Public ranch
	$(call run_pipeline_update,ðŸ’¿ï¸,updating Public ranch pipeline image,testing_farm_public_workers,update_image)

update/redhat/pipeline/image: .FORCE  ## Update pipeline container image on Red Hat ranch
	$(call run_pipeline_update,ðŸ’¿ï¸,updating Red Hat ranch pipeline image,testing_farm_redhat_workers,update_image)

update/pipeline/config: .FORCE  ## Update pipeline configuration on all ranches
	$(call run_pipeline_update,ðŸ› ,updating pipeline configuration on all ranches,testing_farm_public_workers$(comma)testing_farm_redhat_workers,update_config)

update/public/pipeline/config: .FORCE  ## Update pipeline configuration on Public ranch
	$(call run_pipeline_update,ðŸ› ,updating Public ranch pipeline configuration,testing_farm_public_workers,update_config)

update/redhat/pipeline/config: .FORCE  ## Update pipeline configuration on Red Hat ranch
	$(call run_pipeline_update,ðŸ› ,updating Red Hat ranch pipeline configuration,testing_farm_redhat_workers,update_config)

update/pipeline/jobs: .FORCE  ## Update pipeline jobs on all ranches
	$(call run_pipeline_update,â›´ï¸,updating pipeline jobs on all ranches,testing_farm_public_workers$(comma)testing_farm_redhat_workers,update_jobs)

update/public/pipeline/jobs: .FORCE  ## Update pipeline jobs on Public ranch
	$(call run_pipeline_update,â›´ï¸,updating Public ranch pipeline jobs,testing_farm_public_workers,update_jobs)

update/redhat/pipeline/jobs: .FORCE  ## Update pipeline jobs on Red Hat ranch
	$(call run_pipeline_update,â›´ï¸,updating Red Hat ranch pipeline jobs,testing_farm_redhat_workers,update_jobs)


##@ Tests | maintainer

define run_pytest_gluetool
	poetry run pytest $(PYTEST_OPTIONS) $(PYTEST_PARALLEL_OPTIONS) $2 -vvv --basetemp $$PROJECT_ROOT/.pytest \
	--color=yes \
	--citool-extra-podman-args "$(CITOOL_EXTRA_PODMAN_ARGS)" \
	--citool-config terragrunt/environments/$1/worker/citool-config --citool-image $(WORKER_IMAGE) \
	--test-assets tests/worker \
	--variables terragrunt/environments/$1/worker/citool-config/variables.yaml \
	--variables tests/common.yaml \
	--html report.html tests/worker/test_pipeline.py
endef

define download_oculus_to_pytest
	curl -s --fail --retry 5 --show-error https://gitlab.com/testing-farm/oculus/-/raw/main/results.html | tee > /dev/null $$(find .pytest -mindepth 2 -maxdepth 2 -type d -exec echo {}/results.html \;)
endef

test/dev/pipeline: .FORCE wait/artemis/dev generate/dev/citool-config  ## Run worker tests | dev
	$(call run_pytest_gluetool,dev,-m "public and pipeline and not container")
	$(call download_oculus_to_pytest)

test/dev/pipeline/container: .FORCE wait/artemis/dev generate/dev/citool-config  ## Run worker tests | dev
	$(call run_pytest_gluetool,dev,-m "public and pipeline and container")
	$(call download_oculus_to_pytest)

test/dev/compose: .FORCE wait/artemis/dev generate/dev/citool-config  ## Run compose tests | dev
	$(call run_pytest_gluetool,dev,-m "public and compose")
	$(call download_oculus_to_pytest)

test/staging/pipeline: .FORCE wait/artemis/staging generate/staging/citool-config  ## Run worker tests, except container | staging
	$(call run_pytest_gluetool,staging,-m "public and pipeline and not container")
	$(call download_oculus_to_pytest)

test/staging/pipeline/container: .FORCE wait/artemis/staging generate/staging/citool-config  ## Run worker tests, container only | staging
	$(call run_pytest_gluetool,staging,-m "public and pipeline and container")
	$(call download_oculus_to_pytest)

test/staging/pipeline/ci: .FORCE wait/artemis/staging/ci generate/staging/citool-config/ci  ## Run worker tests, except container | staging | CI
	$(call run_pytest_gluetool,staging,-m "public and pipeline and not container")

test/staging/pipeline/container/ci: .FORCE generate/staging/citool-config/container/ci  ## Run worker tests, container only | staging | CI
	$(call run_pytest_gluetool,staging,-m "public and pipeline and container")

test/staging/compose: .FORCE wait/artemis/staging generate/staging/citool-config  ## Run compose tests | staging
	$(call run_pytest_gluetool,staging,-m "public and compose")
	$(call download_oculus_to_pytest)

test/staging/compose/ci: .FORCE wait/artemis/staging/ci generate/staging/citool-config/ci  ## Run compose tests | staging | CI
	$(call run_pytest_gluetool,staging,-m "public and compose")

test/pytest/oculus: .FORCE  ## Download oculus to .pytest directory
	$(call download_oculus_to_pytest)

generate/dev/citool-config: .FORCE  ## Generate citool-config | dev
	poetry run python setup/generate_environment.py dev

generate/staging/citool-config: .FORCE  ## Generate citool-config | staging
	poetry run python setup/generate_environment.py staging

generate/staging/citool-config/ci: .FORCE  ## Generate citool-config | staging | CI
	ARTEMIS_DEPLOYMENT=artemis-ci poetry run python setup/generate_environment.py staging

generate/staging/citool-config/container/ci: .FORCE  ## Generate citool-config for containers | staging | CI
	ARTEMIS_DEPLOYMENT=none poetry run python setup/generate_environment.py staging

generate/public/tests/compose: .FORCE  ## Generate or update compose tests for Public Ranch
	@TESTING_FARM_API_TOKEN=${TESTING_FARM_PRODUCTION_API_TOKEN_PUBLIC} TESTING_FARM_API_URL=${TESTING_FARM_PRODUCTION_API_URL} poetry run python setup/generate_compose_tests.py

list/integration/tests: .FORCE  ## List availableintegration tests
	poetry run pytest $(PYTEST_OPTIONS) -v --basetemp $$PROJECT_ROOT/.pytest --collect-only \
	--citool-config ranch/redhat/citool-config --citool-image $(WORKER_IMAGE) \
	--test-assets tests/worker \
	--html report.html tests/worker/test_pipeline.py


##@ Compose

compose/update/public: .FORCE  ## Update composes in the Public ranch
	poetry run python setup/compose_update_public.py


##@ Utility

edit/secrets: .FORCE  ## Edit secrets software
	ansible-vault edit ansible/secrets/credentials.yaml

$(ENVIRONMENT_FILES):
	# Generate `environment.yaml` variable files
	poetry run python setup/generate_environment.py

	# Decrypt ssh keys
	for key in $(ENVIRONMENT_KEYS); do \
		echo "Decrypting $${key%.decrypted}..."; \
		ansible-vault decrypt --vault-password-file .vault_pass --output $${key} $${key%.decrypted}; \
	done

wait/artemis/dev: .FORCE  ## Wait until Artemis is available | dev
	@bash setup/wait_artemis_available.sh dev

wait/artemis/staging: .FORCE  ## Wait until Artemis is available | staging
	@bash setup/wait_artemis_available.sh staging

wait/artemis/staging/ci: .FORCE  ## Wait until Artemis is available | staging | CI
	@ARTEMIS_DEPLOYMENT=artemis-ci bash setup/wait_artemis_available.sh staging


##@ Cleanup | maintainer

terminate/artemis/guests/dev: .FORCE  ## Terminate all EC2 instances created by Artemis | dev
	@bash $$PROJECT_ROOT/setup/terminate_artemis_guests.sh dev || true

terminate/artemis/guests/staging: .FORCE  ## Terminate all EC2 instances created by Artemis | staging
	@bash $$PROJECT_ROOT/setup/terminate_artemis_guests.sh staging || true

terminate/artemis/guests/staging/ci: .FORCE  ## Terminate all EC2 instances created by Artemis | staging | CI
	@ARTEMIS_DEPLOYMENT=artemis-ci bash $$PROJECT_ROOT/setup/terminate_artemis_guests.sh staging || true

terminate/artemis/guests/production: .FORCE  ## Terminate all EC2 instances created by Artemis | staging
	@bash $$PROJECT_ROOT/setup/terminate_artemis_guests.sh production || true

terminate/eks/ci: .FORCE  ## Terminate all EKS CI clusters
	@bash $$PROJECT_ROOT/setup/terminate_eks_ci_clusters.sh

cleanup/staging/ci: .FORCE staging/kubeconfig  ## Cleanup CI leftovers from staging
	@bash setup/terminate_eks_ci_namespaces.sh
